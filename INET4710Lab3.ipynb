{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#483D8B\">\n",
    "<h1  align=\"center\">Matrix and Graph Data Types</h1>\n",
    "<h2  align=\"center\">Lab 3</h2>\n",
    "<h4 align=\"center\">\n",
    "INET4710 Spring 2019<br>\n",
    "Submitted by Kevin Yang</h4>\n",
    "</font>\n",
    "\n",
    "---------------\n",
    "\n",
    "### Lab Objectives\n",
    "\n",
    "* Write scalable operations using the following data types:\n",
    "    - Vector\n",
    "    - Matrix\n",
    "        - Distributed Matrix\n",
    "    - Graph\n",
    "\n",
    "\n",
    "* Code matrix operations, graph operations, and data type conversions in:\n",
    "    - scipy\n",
    "    - pyspark\n",
    "\n",
    "\n",
    "* Learn a few useful basic machine learning techniques\n",
    "    - Matrix\n",
    "        - SVD (Singular Value Decomposition)\n",
    "        - SGD (Stochastic Gradient Descent)\n",
    "    - Graph\n",
    "        - Find Shortest Distance\n",
    "\n",
    "Instructions: <br>\n",
    "** For this lab, please submit the Jupyter notebook and include answers to the 10 questions. **\n",
    "\n",
    "Notes: <br>\n",
    "- ** scipy.linalg vs numpy.linalg ** <br>\n",
    "from https://docs.scipy.org/doc/scipy/reference/tutorial/linalg.html <br>\n",
    "scipy.linalg contains all the functions in numpy.linalg. plus some other more advanced ones not contained in numpy.linalg. (For this reason, please use scipy.linalg) \n",
    "\n",
    "\n",
    "- ** matrix subclass of ndarray ** <br>\n",
    "from http://currents.soest.hawaii.edu/ocn_data_analysis/numpy_tutorial.html <br>\n",
    "Numpy includes a matrix subclass of the ndarray base class. Please ignore it. It is not commonly used, it provides only small advantages and only under highly restricted circumstances, and elsewhere it can cause hard-to-diagnose problems. (Please use 2D numpy.ndarray objects instead) \n",
    "\n",
    "\n",
    "- ** scipy documentation ** <br>\n",
    "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_SciPy_Cheat_Sheet_Linear_Algebra.pdf <br>\n",
    "https://docs.scipy.org/doc/scipy/reference/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "\n",
    "###  scipy matrix\n",
    "\n",
    "- sparse \n",
    "- dense \n",
    "\n",
    "from https://scipy.github.io/old-wiki/pages/SciPyPackages/Sparse.html <br>\n",
    "A sparse matrix is a two-dimensional matrix with a large number of zero values. In contrast, a matrix where many or most entries are non-zero is said to be dense. There are no strict rules for what constitutes a sparse matrix, so we'll say that a matrix is sparse if there is some benefit to exploiting its sparsity. Additionally, there are a variety of sparse matrix formats which are designed to exploit different sparsity patterns (the structure of non-zero values in a sparse matrix) and different methods for accessing and manipulating matrix entries.\n",
    "\n",
    "Sparsity Patterns:\n",
    "- Diagonal\n",
    "- Block\n",
    "- Unstructured\n",
    "- Sensitivity of pattern to ordering, and use of reordering for locality (e.g. direct solvers)\n",
    "\n",
    "Each sparse format has certain advantages and disadvantages. For instance, adding new non-zero entries to a lil_matrix is fast, however changing the sparsity pattern of a csr_matrix requires a significant amount of work. On the other hand, operations such as matrix-vector multiplication and matrix-matrix arithmetic are much faster with csr_matrix than lil_matrix. A good strategy is to construct matrices using one format and then convert them to another that is better suited for efficient computation.\n",
    "\n",
    "from https://docs.scipy.org/doc/scipy/reference/sparse.html <br/>\n",
    "The scipy.sparse module provides data structures for 2D sparse matrices. There are seven sparse matrix types:\n",
    "\n",
    "- csc_matrix: Compressed Sparse Column format\n",
    "- csr_matrix: Compressed Sparse Row format\n",
    "- bsr_matrix: Block Sparse Row format\n",
    "- lil_matrix: List of Lists format\n",
    "- dok_matrix: Dictionary of Keys format\n",
    "- coo_matrix: COOrdinate format (aka IJV, triplet format)\n",
    "- dia_matrix: DIAgonal format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<1x10 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 6 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# sparse spark vector\n",
    "# demonstrate sample code to create scipy sparse vectors\n",
    "import numpy as np\n",
    "import scipy.sparse# sparse spark vector\n",
    "\n",
    "# code adapted from https://stackoverflow.com/questions/2540059/scipy-sparse-arrays\n",
    "n = 10\n",
    "# create a 50% sparse vector of random numbers\n",
    "x = (np.random.rand(n) * 2).astype(int).astype(float) \n",
    "print( type(x) )\n",
    "\n",
    "# convert numpy vector to csr vector\n",
    "x_csr = scipy.sparse.csr_matrix(x)\n",
    "# convert numpy vector to dok vector\n",
    "x_dok = scipy.sparse.dok_matrix(x.reshape(x_csr.shape))\n",
    "\n",
    "print ( repr(x_csr) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix multiplication: numpy\n",
      "0.0031328719999998977\n",
      "\n",
      "matrix multiplication: dictionary of keys\n",
      "0.3467509600000003\n",
      "\n",
      "matrix multiplication: compressed sparse row multiply sum\n",
      "0.026362887000000335\n",
      "\n",
      "matrix multiplication: compressed sparse row transpose\n",
      "0.021662726000000188\n"
     ]
    }
   ],
   "source": [
    "# sparse spark vector\n",
    "# performance comparison of sparse data structures\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "# create a 50% sparse vector of random numbers\n",
    "def mm_numpy():\n",
    "    n = 1000\n",
    "    x = (np.random.rand(n) * 2).astype(int).astype(float) \n",
    "    np.dot(x, x)\n",
    "\n",
    "def mm_dict():\n",
    "    n = 1000\n",
    "    x = (np.random.rand(n) * 2).astype(int).astype(float) \n",
    "    x_csr = scipy.sparse.csr_matrix(x)\n",
    "    x_dok = scipy.sparse.dok_matrix(x.reshape(x_csr.shape))\n",
    "    x_dok * x_dok.T\n",
    "\n",
    "def mm_csrmult():\n",
    "    n = 1000\n",
    "    x = (np.random.rand(n) * 2).astype(int).astype(float) \n",
    "    x_csr = scipy.sparse.csr_matrix(x)\n",
    "    x_csr.multiply(x_csr).sum()\n",
    "    \n",
    "def mm_csrdot():\n",
    "    n = 1000\n",
    "    x = (np.random.rand(n) * 2).astype(int).astype(float) \n",
    "    x_csr = scipy.sparse.csr_matrix(x)\n",
    "    x_csr * x_csr.T\n",
    "    \n",
    "print (\"matrix multiplication: numpy\")\n",
    "t = timeit.timeit(\"mm_numpy()\", setup='import numpy as np; from __main__ import mm_numpy', number=50)\n",
    "print ( t )\n",
    "\n",
    "print (\"\\nmatrix multiplication: dictionary of keys\")\n",
    "t = timeit.timeit('mm_dict()', setup='import numpy as np; from __main__ import mm_dict', number=50)\n",
    "print ( t )\n",
    "\n",
    "print (\"\\nmatrix multiplication: compressed sparse row multiply sum\")\n",
    "t = timeit.timeit(\"mm_csrmult()\", setup='import numpy as np; from __main__ import mm_csrmult', number=50)\n",
    "print ( t )\n",
    "\n",
    "print (\"\\nmatrix multiplication: compressed sparse row transpose\")\n",
    "t = timeit.timeit(\"mm_csrdot()\", setup='import numpy as np; from __main__ import mm_csrdot', number=50)\n",
    "print ( t )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1. Modify the sparse vector performance code above to create two-dimensional matrices and to loop through a few iterations of different sized matrices. Plot the result (matrix size or iteration number vs. time).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# answer to question 1\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "import timeit\n",
    "import pandas as pd\n",
    "\n",
    "#different sizes of n for each matrix\n",
    "\n",
    "def mm_numpy():\n",
    "    n = 300\n",
    "    x = (np.random.rand(n) * 2).astype(int).astype(float) \n",
    "    np.dot(x, x)\n",
    "\n",
    "def mm_dict():\n",
    "    n = 500\n",
    "    x = (np.random.rand(n) * 2).astype(int).astype(float) \n",
    "    x_csr = scipy.sparse.csr_matrix(x)\n",
    "    x_dok = scipy.sparse.dok_matrix(x.reshape(x_csr.shape))\n",
    "    x_dok * x_dok.T\n",
    "\n",
    "def mm_csrmult():\n",
    "    n = 700\n",
    "    x = (np.random.rand(n) * 2).astype(int).astype(float) \n",
    "    x_csr = scipy.sparse.csr_matrix(x)\n",
    "    x_csr.multiply(x_csr).sum()\n",
    "    \n",
    "def mm_csrdot():\n",
    "    n = 900\n",
    "    x = (np.random.rand(n) * 2).astype(int).astype(float) \n",
    "    x_csr = scipy.sparse.csr_matrix(x)\n",
    "    x_csr * x_csr.T\n",
    "\n",
    "mainlist = []\n",
    "alist = []    \n",
    "for i in  range(50):\n",
    "    t_1 = timeit.timeit(\"mm_numpy()\", setup='import numpy as np; from __main__ import mm_numpy', number=50)\n",
    "    alist.append(t_1)\n",
    "    \n",
    "mainlist.append(alist)\n",
    "blist = []\n",
    "\n",
    "for i in  range(50):    \n",
    "    t_2 = timeit.timeit('mm_dict()', setup='import numpy as np; from __main__ import mm_dict', number=50)\n",
    "    blist.append(t_2)\n",
    "    \n",
    "mainlist.append(blist)\n",
    "clist = []\n",
    "\n",
    "for i in range(50): \n",
    "    t_3 = timeit.timeit(\"mm_csrmult()\", setup='import numpy as np; from __main__ import mm_csrmult', number=50)\n",
    "    clist.append(t_3)\n",
    "\n",
    "mainlist.append(clist)\n",
    "dlist = []\n",
    "\n",
    "for i in range(50):\n",
    "    t_4 = timeit.timeit(\"mm_csrdot()\", setup='import numpy as np; from __main__ import mm_csrdot', number=50)\n",
    "    dlist.append(t_4)\n",
    "    \n",
    "mainlist.append(dlist)\n",
    "\n",
    "plt.plot(mainlist)\n",
    "plt.ylabel('Time') # Tvalues for each matrix\n",
    "plt.xlabel('Iteration Number') # 50\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2. Convert matrix A to CSR format and solve (A A^T) x = b for x.\n",
    "Output the first 10 elements of x.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1196.5174080336653\n",
      "-1557.2342316149022\n",
      "-52.11646593595004\n",
      "-2692.4800369403065\n",
      "-949.5072175461316\n",
      "-3710.7251879613914\n",
      "-348.86746385408253\n",
      "-1681.5781114626427\n",
      "-620.2331003848724\n",
      "-397.6635857918752\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "import scipy.sparse.linalg.dsolve as linsolve\n",
    "from numpy import random, linalg\n",
    "\n",
    "# answer to question 2\n",
    "#mat_A = matrix(1000,1000)\n",
    "\n",
    "mat_A = sparse.lil_matrix((1000,1000))\n",
    "mat_A[0, :100] = random.rand(100)\n",
    "mat_A[1, 100:200] = mat_A[0, :100]\n",
    "mat_A.setdiag(random.rand(1000))\n",
    "    \n",
    "mat_A = sparse.csr_matrix(mat_A) #A.tocsr()\n",
    "b = random.rand(1000)\n",
    "x = linsolve.spsolve(mat_A * mat_A.T, b)\n",
    "\n",
    "#print(x[:10])  printing this wouldn't notify us which x is 1st,2nd,3rd,etc.,\n",
    "#which is why I printed them separately\n",
    "\n",
    "print(x[0])\n",
    "print(x[1])\n",
    "print(x[2])\n",
    "print(x[3])\n",
    "print(x[4])\n",
    "print(x[5])\n",
    "print(x[6])\n",
    "print(x[7])\n",
    "print(x[8])\n",
    "print(x[9])\n",
    "\n",
    "#A_ = mat_A.todense()\n",
    "#x_ = linalg.solve(A_ * A_.T, b)\n",
    "\n",
    "#err = linalg.norm(x-x_)\n",
    "\n",
    "#print(\"Norm error =\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3. matrix conversion of numpy to/from scipy. \n",
    "Write code in the cell below to do what the comments describe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A - coo matrix format\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "  (0, 0)\t1\n",
      "  (0, 2)\t2\n",
      "  (1, 1)\t3\n",
      "  (3, 3)\t4\n",
      "  (1, 1)\t5\n",
      "  (0, 0)\t6\n",
      "  (0, 0)\t7\n",
      "\n",
      "A - csr matrix\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 0)\t14\n",
      "  (0, 2)\t2\n",
      "  (1, 1)\t8\n",
      "  (3, 3)\t4\n",
      "\n",
      "A - dense matrix format\n",
      "<class 'numpy.matrixlib.defmatrix.matrix'>\n",
      "[[14  0  2  0]\n",
      " [ 0  8  0  0]\n",
      " [ 0  0  0  0]\n",
      " [ 0  0  0  4]]\n",
      "\n",
      "A - ndarray format\n",
      "<class 'numpy.ndarray'>\n",
      "[14  0  2  0  0  8  0  0  0  0  0  0  0  0  0  4]\n"
     ]
    }
   ],
   "source": [
    "# matrix conversion of numpy to/from scipy \n",
    "from numpy import array\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# construct a coo matrix\n",
    "row  = array([0,0,1,3,1,0,0])\n",
    "col  = array([0,2,1,3,1,0,0])\n",
    "data = array([1,2,3,4,5,6,7])\n",
    "A = coo_matrix( (data,(row,col)), shape=(4,4))\n",
    "\n",
    "# output python type and contents of A\n",
    "print (\"A - coo matrix format\")\n",
    "print (type(A))\n",
    "print (A)\n",
    "\n",
    "# question 3 - write code to answer the following:\n",
    "\n",
    "# 1. write code to convert A from coo to csr\n",
    "\n",
    "A = scipy.sparse.csr_matrix(A)\n",
    "\n",
    "# output python type and contents of A in sorted order\n",
    "# note: CSR column indices are not necessarily sorted\n",
    "print (\"\\nA - csr matrix\")\n",
    "print(type(A))\n",
    "A.sort_indices()\n",
    "print(A)\n",
    "\n",
    "# 2. write code to convert A from csr to dense\n",
    "\n",
    "A = A.todense()\n",
    "\n",
    "# output python type and contents of A\n",
    "print (\"\\nA - dense matrix format\")\n",
    "print(type(A))\n",
    "print(A)\n",
    "  \n",
    "# 3. write code to convert dense matrix to ndarray\n",
    "# see https://stackoverflow.com/questions/5183533/how-to-make-list-from-numpy-matrix-in-python\n",
    "\n",
    "A = A.getA1()\n",
    "\n",
    "print (\"\\nA - ndarray format\")\n",
    "print(type(A))\n",
    "print(A)\n",
    "# output python type and contents of A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "\n",
    "###  pyspark local vector and local matrix\n",
    "\n",
    "- sparse vector\n",
    "- dense vector\n",
    "- sparse matrix \n",
    "- dense matrix (use with caution - watch big data sizes and performance)\n",
    "\n",
    "<br>\n",
    "** Local Vector: ** <br>\n",
    "Generally use the following local vector data types in pyspark 2:\n",
    "- Dense vectors (local):\n",
    "    - numpy ndarray, scipy 1D matrix, python list\n",
    "    - pyspark ml DenseVector\n",
    "- Sparse vectors (local):\n",
    "    - scipy 1D sparse matrix\n",
    "    - pyspark ml SparseVector\n",
    "\n",
    "\n",
    "Both dense vector and sparse vector are homogeneous and can only have numeric data\n",
    "\n",
    "pyspark ml machine learning models also use distributed vectors (and matrices). The distributed structures are introduced later in this lab. Distributed matrices are accessed through the pyspark DataFrame data type, built on top of RDDs.\n",
    "\n",
    "\n",
    "adapted from https://github.com/apache/spark/blob/master/docs/mllib-data-types.md <br>\n",
    "A pyspark local vector has integer-typed and 0-based indices and double-typed values, stored on a single machine. The ml module supports two types of local vectors: dense and sparse. A dense vector is backed by a double array representing its entry values, while a sparse vector is backed by two parallel arrays: indices and values. \n",
    "\n",
    "For example, a vector (1.0, 0.0, 3.0) can be represented in dense format as [1.0, 0.0, 3.0] or in sparse format as (3, [0, 2], [1.0, 3.0]), where 3 is the size of the vector.\n",
    "\n",
    "The base class of local vectors is Vector, and we provide two implementations: DenseVector and SparseVector. We recommend using the factory methods implemented in Vectors to create local vectors.\n",
    "\n",
    "DenseVector behaves similarly to a numpy.ndarray or python list, and generally these structures can be used interchangably in pyspark\n",
    "\n",
    "<br>\n",
    "** Local Matrix ** <br>\n",
    "A local matrix has integer-typed row and column indices and double-typed values, stored on a single machine. pyspark ml module supports dense matrices, in which element values are stored in a single double array in column major.\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "###  pyspark data frame\n",
    "\n",
    "from https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/ <br>\n",
    "A DataFrame is a distributed collection of rows under named columns. <br>\n",
    "In simple terms, it is similar to a table in relational database or an Excel sheet with Column headers. <br>\n",
    "It also shares some common characteristics with RDD:\n",
    "\n",
    "- Immutable in nature : \n",
    "    - We can create DataFrame / RDD once but can’t change it.\n",
    "    - And we can transform a DataFrame / RDD  after applying transformations.\n",
    "- Lazy Evaluations: Which means that a task is not executed until an action is performed.\n",
    "- Distributed: RDD and DataFrame both are distributed in nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# set up pyspark environment\n",
    "import findspark\n",
    "# initialize - specify SPARK_HOME\n",
    "findspark.init('C:\\Spark\\spark-2.3.2-bin-hadoop2.7')\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "# local[*]  Run Spark locally with as many worker threads \n",
    "#           as logical cores on your machine\n",
    "# getOrCreate() Gets an existing SparkSession or, if there \n",
    "#           is no existing one, creates a new one based on the \n",
    "#           options set in this builder\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"lab1\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.901688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1         2   3   4\n",
       "0 NaN NaN  0.901688 NaN NaN\n",
       "1 NaN NaN       NaN NaN NaN\n",
       "2 NaN NaN       NaN NaN NaN\n",
       "3 NaN NaN       NaN NaN NaN\n",
       "4 NaN NaN       NaN NaN NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scipy sparse csr matrix -> pandas sparse data frame\n",
    "# demonstrate example code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "arr = np.random.random(size=(1000, 5))\n",
    "arr[arr < .9] = 0\n",
    "sp_arr = csr_matrix(arr)\n",
    "sdf = pd.SparseDataFrame(sp_arr)\n",
    "sdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[0: double, 1: double, 2: double, 3: double, 4: double]\n",
      "+---+---+------------------+---+---+\n",
      "|  0|  1|                 2|  3|  4|\n",
      "+---+---+------------------+---+---+\n",
      "|NaN|NaN|0.9016880875957054|NaN|NaN|\n",
      "|NaN|NaN|               NaN|NaN|NaN|\n",
      "|NaN|NaN|               NaN|NaN|NaN|\n",
      "|NaN|NaN|               NaN|NaN|NaN|\n",
      "|NaN|NaN|               NaN|NaN|NaN|\n",
      "+---+---+------------------+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pandas sparse data frame -> pyspark data frame\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "spark_df = sqlContext.createDataFrame(sdf)\n",
    "\n",
    "# output data types in pyspark data frame\n",
    "print( spark_df )\n",
    "spark_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4. Starting with the ndarray below:**\n",
    "1. **Convert from the numpy ndarray to a pandas data frame**\n",
    "2. **Label the columns in the pandas data frame \"random_sample1\" and \"random_sample2\"**\n",
    "3. **Print the pyspark data frame schema**\n",
    "4. **Output the first 10 lines in the pyspark data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[0: double, 1: double]\n",
      "root\n",
      " |-- 0: double (nullable = true)\n",
      " |-- 1: double (nullable = true)\n",
      "\n",
      "+---+------------------+\n",
      "|  0|                 1|\n",
      "+---+------------------+\n",
      "|0.0|               0.0|\n",
      "|0.0|0.9442505391032223|\n",
      "|0.0|               0.0|\n",
      "|0.0|               0.0|\n",
      "|0.0|0.9012263281548435|\n",
      "|0.0|               0.0|\n",
      "|0.0|               0.0|\n",
      "|0.0|0.9507400380308756|\n",
      "|0.0|               0.0|\n",
      "|0.0|               0.0|\n",
      "+---+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lab exercise: convert numpy array to pandas data frame \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "nparr = np.random.random(size=(1000, 2))\n",
    "nparr[nparr < .9] = 0\n",
    "\n",
    "# convert the numpy ndarray to a pandas data frame\n",
    "\n",
    "#spx_arr = csr_matrix(nparr)\n",
    "spdf = pd.DataFrame(nparr)\n",
    "\n",
    "# label the feature (i.e., assign a name to the column) in the pandas data frame\n",
    "#spk_df.rename(columns={0:\"A\",1:\"B\"})\n",
    "spdf.rename(columns={0:\"Col_1\",1:\"Col_2\"})\n",
    "\n",
    "# convert the pandas data frame to a pyspark data frame\n",
    "sqlC = SQLContext(sc)\n",
    "spk_df = sqlC.createDataFrame(spdf)\n",
    "\n",
    "# print the pyspark data frame schema\n",
    "print(spk_df)\n",
    "spk_df.printSchema()\n",
    "# output the first 10 rows in the pyspark data frame\n",
    "spk_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# pyspark sparse vector -> numpy dense vector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "sparseVector = Vectors.sparse(10, [1, 3], [3.0, 4.5])\n",
    "denseVector = sparseVector.toArray()\n",
    "print ( type(denseVector))\n",
    "\n",
    "# Why didn't we convert from a pyspark sparse vector -> spark dense vector\n",
    "# https://forums.databricks.com/questions/8895/converting-dataframe-sparse-vector-column-to-dense.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
      "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.40047142, 0.91359586,        nan, 1.        ]])\n",
      " \n",
      "Spearman correlation matrix:\n",
      "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
      "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE pyspark data frame contructed from sparse and dense vectors\n",
    "# https://spark.apache.org/docs/latest/ml-statistics.html\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "print(\" \")\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "\n",
    "###  pyspark distributed matrix operations\n",
    "\n",
    "In Spark 2.0 you have to use correct local types:\n",
    "-\tpyspark.ml.linalg when working DataFrame based pyspark.ml API.\n",
    "-\tpyspark.mllib.linalg when working RDD based pyspark.mllib API.\n",
    "These two namespaces are no longer compatible and require explicit conversions\n",
    "\n",
    "<br>\n",
    "An aside: <br>\n",
    "https://home.apache.org/~pwendell/spark-nightly/spark-branch-2.0-docs/latest/ml-guide.html <br>\n",
    "The MLlib RDD-based API is now in maintenance mode.\n",
    "\n",
    "As of Spark 2.0, the RDD-based APIs in the spark.mllib package have entered maintenance mode. The primary Machine Learning API for Spark is now the DataFrame-based API in the spark.ml package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csc.csc_matrix'>\n",
      "[[1 0 4]\n",
      " [0 0 5]\n",
      " [2 3 6]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "[1 0 4]\n",
      "[0 0 5]\n",
      "[2 3 6]\n",
      "For large RDD data sizes, better to output a sample number of rows from RDD\n",
      "[1 0 4]\n",
      "[0 0 5]\n"
     ]
    }
   ],
   "source": [
    "#  scipy sparse matrix to pyspark RDD\n",
    "# https://stackoverflow.com/questions/40645498/create-sparse-rdd-from-scipy-sparse-matrix/40648106#40648106\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "\n",
    "# create a sparse matrix\n",
    "row = np.array([0, 2, 2, 0, 1, 2])\n",
    "col = np.array([0, 0, 1, 2, 2, 2])\n",
    "data = np.array([1, 2, 3, 4, 5, 6]) \n",
    "sv = sps.csc_matrix((data, (row, col)), shape=(3, 3))\n",
    "print ( type(sv) )\n",
    "nparray = sv.toarray()\n",
    "print (nparray)\n",
    "\n",
    "print ( type(nparray) )\n",
    "\n",
    "#read sv as RDD\n",
    "sv_rdd = sc.parallelize(sv.toarray())  #convert scipy csc matrix to RDD\n",
    "\n",
    "print( type(sv_rdd) )\n",
    "for x in sv_rdd.collect():\n",
    "    print (x)\n",
    "print (\"For large RDD data sizes, better to output a sample number of rows from RDD\")\n",
    "for x in sv_rdd.take(2):\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5. Why was it necessary to execute collect() before printing each array element?** <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to execute collect() because data is being loaded into the RAM or the driver memory, which makes it faster and more reliable than retrieving data from a disk.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[((0, 0), DenseMatrix(3, 3, [17.0, 22.0, 27.0, 22.0, 29.0, 36.0, 27.0, 36.0, 45.0], 0)), ((1, 0), DenseMatrix(3, 3, [47.0, 52.0, 57.0, 64.0, 71.0, 78.0, 81.0, 90.0, 99.0], 0)), ((0, 1), DenseMatrix(3, 3, [47.0, 64.0, 81.0, 52.0, 71.0, 90.0, 57.0, 78.0, 99.0], 0)), ((1, 1), DenseMatrix(3, 3, [149.0, 166.0, 183.0, 166.0, 185.0, 204.0, 183.0, 204.0, 225.0], 0))]\n"
     ]
    }
   ],
   "source": [
    "# pyspark multiplication using RDD BlockMatrix \n",
    "from pyspark.mllib.linalg import Matrices\n",
    "from pyspark.mllib.linalg.distributed import BlockMatrix\n",
    "\n",
    "# Create an RDD of sub-matrix blocks.\n",
    "blocks = sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])), \n",
    "                         ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))])\n",
    "\n",
    "# Create a BlockMatrix from an RDD of sub-matrix blocks.\n",
    "matA = BlockMatrix(blocks, 3, 2)\n",
    "matB = BlockMatrix(blocks, 3, 2).transpose()\n",
    "\n",
    "# colsPerBlock of this matrix must equal the rowsPerBlock of other\n",
    "amultb = matA.multiply(matB)\n",
    "adotb = amultb.blocks.collect()\n",
    "print( type(adotb) )\n",
    "print( adotb )\n",
    "\n",
    "# note: an improvement over this method is given in:\n",
    "# https://labs.yodas.com/large-scale-matrix-multiplication-with-pyspark-or-how-to-match-two-large-datasets-of-company-1be4b1b2871e\n",
    "# which is much more involved than the BlockMatrix matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6. Write code in the cell below to execute a pyspark BlockMatrix transpose operation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((0, 0), DenseMatrix(2, 3, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 1)), ((0, 1), DenseMatrix(2, 3, [7.0, 8.0, 9.0, 10.0, 11.0, 12.0], 1))]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "from pyspark.mllib.linalg.distributed import BlockMatrix\n",
    "\n",
    "# Create an RDD of sub-matrix blocks.\n",
    "blocks = sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])), \n",
    "                         ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))])\n",
    "\n",
    "# Create a BlockMatrix from an RDD of sub-matrix blocks.\n",
    "matA = BlockMatrix(blocks, 3, 2)\n",
    "\n",
    "# write code to transpose matA\n",
    "matA_T = matA.transpose()\n",
    "# output transposed matrix elements\n",
    "\n",
    "matA_Tcollect = matA_T.blocks.collect()\n",
    "print( matA_Tcollect )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7. Write code in the cell below to execute a pyspark SVD operation and output the U, s, and V results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The U factor:  <pyspark.mllib.linalg.distributed.RowMatrix object at 0x00000180EFEA4160> \n",
      "\n",
      "[-0.38829130511665644,-0.9198099362554474,-0.056387441301709175,9.313225746154785e-09,0.0]\n",
      "[-0.5301719995198351,0.2730185511901228,-0.8027319114319463,0.0,0.0]\n",
      "[-0.7537556058139434,0.2817987790459642,0.5936682026454339,1.4901161193847656e-08,1.4901161193847656e-08]\n",
      "\n",
      "The singular values:  [13.029275535600473,5.368578733451684,2.5330498218813755,6.323166049206486e-08,2.0226934557075942e-08] \n",
      "\n",
      "\n",
      "The V factor:  DenseMatrix([[-0.31278534,  0.31167136,  0.30366911,  0.8409913 , -0.07446478],\n",
      "             [-0.02980145, -0.17133211, -0.02226069,  0.14664984,  0.97352733],\n",
      "             [-0.12207248,  0.15256471, -0.95070998,  0.23828799, -0.03452092],\n",
      "             [-0.71847899, -0.68096285, -0.0172245 , -0.02094998, -0.13907533],\n",
      "             [-0.60841059,  0.62170723,  0.05606596, -0.46260933,  0.16175873]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVD (Singular Value Decomposition)\n",
    "# from https://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "rows = sc.parallelize([\n",
    "    Vectors.sparse(5, {1: 1.0, 3: 7.0}),\n",
    "    Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "    Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    "])\n",
    "\n",
    "mat = RowMatrix(rows)\n",
    "\n",
    "# Compute the top 5 singular values and corresponding singular vectors.\n",
    "svd = mat.computeSVD(5, computeU=True)\n",
    "U = svd.U       # The U factor is a RowMatrix.\n",
    "s = svd.s       # The singular values are stored in a local dense vector.\n",
    "V = svd.V       # The V factor is a local dense matrix.\n",
    "\n",
    "U_collect = U.rows.collect()\n",
    "print(\"The U factor: \",U,\"\\n\")\n",
    "for i in U_collect:\n",
    "    print(i)\n",
    "\n",
    "print(\"\\nThe singular values: \",s,\"\\n\")\n",
    "print(\"\\nThe V factor: \",V,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8. Write code in the cell below to execute a pyspark SGD operation and output the result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-16-20d6a8cddbd5>, line 71)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-20d6a8cddbd5>\"\u001b[1;36m, line \u001b[1;32m71\u001b[0m\n\u001b[1;33m    \"Usage: PythonLR <master> <iterations> [<slices>]\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# SGD (Stochastic Gradient Descent)\n",
    "# example - https://gist.github.com/MLnick/4707012\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "from numpy import array, random as np_random\n",
    "\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "from sklearn.base import copy\n",
    "\n",
    "\n",
    "\n",
    "N = 10000   # Number of data points\n",
    "\n",
    "D = 10      # Numer of dimensions\n",
    "\n",
    "ITERATIONS = 5\n",
    "\n",
    "np_random.seed(seed=42)\n",
    "\n",
    "\n",
    "\n",
    "def generate_data(N):\n",
    "\n",
    "    return [[1 if np_random.rand() < 0.5 else 0, np_random.randn(1, D)] for ii in range(N)]\n",
    "\n",
    "\n",
    "\n",
    "def train(iterator, sgd):\n",
    "\n",
    "    for x in iterator:\n",
    "\n",
    "        sgd.partial_fit(x[1], [x[0]], classes=array([0, 1]))\n",
    "    yield sgd\n",
    "\n",
    "\n",
    "\n",
    "def merge(left, right):\n",
    "\n",
    "    new = copy.deepcopy(left)\n",
    "\n",
    "    new.coef_ += right.coef_\n",
    "\n",
    "    new.intercept_ += right.intercept_\n",
    "\n",
    "    return new\n",
    "\n",
    "\n",
    "\n",
    "def avg_model(sgd, slices):\n",
    "\n",
    "    sgd.coef_ /= slices\n",
    "\n",
    "    sgd.intercept_ /= slices\n",
    "\n",
    "    return sgd\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if len(sys.argv) < 2:\n",
    "\n",
    "        print >> sys.stderr, \\\n",
    "\n",
    "            \"Usage: PythonLR <master> <iterations> [<slices>]\"\n",
    "\n",
    "        exit(-1)\n",
    "\n",
    "    #print sys.argv\n",
    "\n",
    "\n",
    "\n",
    "    sc = SparkContext(sys.argv[1], \"PythonLR\")\n",
    "\n",
    "    ITERATIONS = int(sys.argv[2]) if len(sys.argv) > 2 else ITERATIONS\n",
    "\n",
    "    slices = int(sys.argv[3]) if len(sys.argv) == 4 else 2\n",
    "\n",
    "    data = generate_data(N)\n",
    "\n",
    "    print len(data)\n",
    "\n",
    "\n",
    "\n",
    "    # init stochastic gradient descent\n",
    "\n",
    "    sgd = lm.SGDClassifier(loss='log')\n",
    "\n",
    "    # training\n",
    "\n",
    "    for ii in range(ITERATIONS):\n",
    "\n",
    "        sgd = sc.parallelize(data, numSlices=slices) \\\n",
    "\n",
    "                .mapPartitions(lambda x: train(x, sgd)) \\\n",
    "\n",
    "                .reduce(lambda x, y: merge(x, y))\n",
    "\n",
    "        sgd = avg_model(sgd, slices) # averaging weight vector => iterative parameter mixtures\n",
    "\n",
    "        print \"Iteration %d:\" % (ii + 1)\n",
    "\n",
    "        print \"Model: \"\n",
    "\n",
    "        print sgd.coef_\n",
    "\n",
    "        print sgd.intercept_\n",
    "\n",
    "        print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9. Complete the pyspark sql code below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "department1\n",
      "Row(id='123456', name='Computer Science')\n",
      "employee2\n",
      "Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000)\n",
      "email\n",
      "no-reply@berkeley.edu\n",
      "df1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[department: struct<id:string,name:string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:bigint>>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[department: struct<id:string,name:string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:bigint>>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Row\n",
      "\n",
      "Row(department=Row(id='123456', name='Computer Science'), employees=[Row(firstName='michael', lastName='armbrust', email='no-reply@berkeley.edu', salary=100000), Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000)])\n",
      "\n",
      "union\n",
      "\n",
      "department column\n",
      "+--------------------+\n",
      "|          department|\n",
      "+--------------------+\n",
      "|[123456, Computer...|\n",
      "|[789012, Mechanic...|\n",
      "+--------------------+\n",
      "\n",
      "\n",
      "orderBy\n",
      "\n",
      "first name\n",
      "+-------------------+\n",
      "|          firstName|\n",
      "+-------------------+\n",
      "|[michael, xiangrui]|\n",
      "|           [matei,]|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark sql\n",
    "# from https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import col, asc\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# Create Example Data - Departments and Employees\n",
    "\n",
    "# Create the Departments\n",
    "department1 = Row(id='123456', name='Computer Science')\n",
    "department2 = Row(id='789012', name='Mechanical Engineering')\n",
    "department3 = Row(id='345678', name='Theater and Drama')\n",
    "department4 = Row(id='901234', name='Indoor Recreation')\n",
    "\n",
    "# Create the Employees\n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\n",
    "employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
    "employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\n",
    "employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n",
    "\n",
    "# Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\n",
    "departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
    "departmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4])\n",
    "departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n",
    "\n",
    "print(\"department1\")\n",
    "print (department1)\n",
    "print(\"employee2\")\n",
    "print (employee2)\n",
    "print(\"email\")\n",
    "print (departmentWithEmployees1.employees[0].email)\n",
    "\n",
    "departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\n",
    "df1 = sqlContext.createDataFrame(departmentsWithEmployeesSeq1)\n",
    "print(\"df1\")\n",
    "display(df1)\n",
    "\n",
    "departmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]\n",
    "df2 = sqlContext.createDataFrame(departmentsWithEmployeesSeq2)\n",
    "print(\"df2\")\n",
    "display(df2)\n",
    "\n",
    "# 1. write code to output the first row of df1\n",
    "\n",
    "print(\"First Row\\n\")\n",
    "print(df1.first())\n",
    "\n",
    "\n",
    "# 2. write code to calculate the union of df1 and df2\n",
    "print(\"\\nunion\")\n",
    "union_DF = df1.unionAll(df2)\n",
    "#display(union_DF)\n",
    "\n",
    "# 3. write code to select the department column from df1\n",
    "print(\"\\ndepartment column\")\n",
    "df = df1.select(\"department\")\n",
    "df.show()\n",
    "#explodeDF = df.selectExpr(\"d.id\", \"d.name\") ##\"\"e.lastName\", \"e.email\", \"e.salary\"\n",
    "\n",
    "# 4. write code to output df1 sorted by department name (in ascending order)\n",
    "print(\"\\norderBy\")\n",
    "#filterDF = df.sort(asc(\"department\"))\n",
    "#filterDF = df.filter(df,name).sort(asc(\"name\"))\n",
    "#display(whereDF)\n",
    "#filterDF.show()\n",
    "\n",
    "# 5. write code to select employees.firstName\n",
    "print(\"\\nfirst name\")\n",
    "\n",
    "first_name = df1.select(\"employees\")\n",
    "first_sel = first_name.selectExpr(\"employees.firstName\")\n",
    "first_sel.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "### graph\n",
    "\n",
    "Calculate single source shortest distances in O(V+E) time for DAGs using Topological Sorting.\n",
    "\n",
    "Initialize distances to all vertices as infinite and distance to source as 0, then find a topological sorting of the graph. Topological Sorting of a graph represents a linear ordering of the graph. Once a topological order is established, process each vertex in topological order. For every vertex processed, update distances of its adjacent using distance of current vertex.\n",
    "\n",
    "How is the graph represented in the shortest path program?\n",
    "\n",
    "The graph is represented using an adjacency list. Each node of adjacency list contains vertex number of the vertex to which edge connects. It also contains weight of the edge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph - find shortest distance\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self,vertices):\n",
    "        self.V = vertices\n",
    "        self.graph = defaultdict(list)\n",
    "        \n",
    "    def addEdge(self,u,v,w):\n",
    "        self.graph[u].append((v,w))\n",
    " \n",
    "    def topologicalSortUtil(self,v,visited,stack):\n",
    "        visited[v] = True\n",
    " \n",
    "        if v in self.graph.keys():\n",
    "            for node,weight in self.graph[v]:\n",
    "                if visited[node] == False:\n",
    "                    self.topologicalSortUtil(node,visited,stack)\n",
    " \n",
    "        stack.append(v)\n",
    " \n",
    "    def shortestPath(self, s): \n",
    "        visited = [False]*self.V\n",
    "        stack =[]\n",
    "\n",
    "        for i in range(self.V):\n",
    "            if visited[i] == False:\n",
    "                self.topologicalSortUtil(s,visited,stack)\n",
    "        \n",
    "        dist = [float(\"Inf\")] * (self.V)\n",
    "        dist[s] = 0\n",
    " \n",
    "        while stack:\n",
    "            i = stack.pop()\n",
    "            for node,weight in self.graph[i]:\n",
    "                if dist[node] > dist[i] + weight:\n",
    "                    dist[node] = dist[i] + weight\n",
    "        \n",
    "        for i in range(self.V):\n",
    "            if dist[i] == float(\"Inf\"):\n",
    "                print(str(i) + \" : Inf\")\n",
    "            else:\n",
    "                print(str(i) + \" : \" + str(dist[i]))\n",
    "\n",
    "g = Graph(6)\n",
    "g.addEdge(0, 1, 5)\n",
    "g.addEdge(0, 2, 3)\n",
    "g.addEdge(1, 3, 6)\n",
    "g.addEdge(1, 2, 2)\n",
    "g.addEdge(2, 4, 4)\n",
    "g.addEdge(2, 5, 2)\n",
    "g.addEdge(2, 3, 7)\n",
    "g.addEdge(3, 4, -1)\n",
    "g.addEdge(4, 5, -2)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10. Write code (using the python class above) to find the shortest path distances from vertex 1 to other nodes;  output each node and distance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following are shortest distances from source 1 \n",
      "0 : Inf\n",
      "1 : 0\n",
      "2 : 2\n",
      "3 : 6\n",
      "4 : 5\n",
      "5 : 3\n"
     ]
    }
   ],
   "source": [
    "vertex = 1\n",
    "  \n",
    "print (\"Following are shortest distances from source %d \" % vertex) \n",
    "g.shortestPath(vertex) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
