{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#483D8B\">\n",
    "<h1  align=\"center\">Graph Representations</h1>\n",
    "<h2  align=\"center\">Analysis 2</h2>\n",
    "<h4 align=\"center\">\n",
    "INET4710 Spring 2019<br>\n",
    "Submitted by Kevin Yang</h4>\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand and practice implementing different Graph Representations. \n",
    "Practice implementing word models such as the Continuous Bag of Words Model and the Skip gram model. \n",
    "Analyze the Robert Frost poem \"Stopping by Woods on a Snowy Evening.\"\n",
    "\n",
    "Reference:\n",
    "1. Lab 4 document: https://docs.google.com/document/d/10IUkDKQIGfWyvyyOWfTGLsTOIdX1QTwPUdaZ4tf2K0g/edit#\n",
    "2. https://becominghuman.ai/how-does-word2vecs-skip-gram-work-f92e0525def4\n",
    "3. https://skymind.ai/wiki/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kyang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Reads ‘poem.txt’ file \n",
    "sample = open(\"C:\\\\Users\\\\kyang\\\\Desktop\\\\DataSci2\\\\L4\\\\poem.txt\", \"r\") \n",
    "s = sample.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'poem.txt' file will be stored in 'sample' for the Continuous Bag of Words Model and the Skip Gram Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces escape character with space \n",
    "f = s.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(f): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, I create the Continous Bag of Words Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'woods' and 'sleep' - CBOW :  0.14891554\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity: Woods, Sleep\n",
    "print(\"Cosine similarity between 'woods' \" + \n",
    "               \"and 'sleep' - CBOW : \", \n",
    "    model1.similarity('woods', 'sleep')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'i' and 'he' - CBOW :  0.009882852\n"
     ]
    }
   ],
   "source": [
    "# Print results \n",
    "print(\"Cosine similarity between 'i' \" + \n",
    "               \"and 'he' - CBOW : \", \n",
    "    model1.similarity('i', 'he')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "                                             window = 5, sg = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'woods' and 'sleep' - Skip Gram :  0.14967775\n"
     ]
    }
   ],
   "source": [
    "# Skip Gram : Woods, Sleep\n",
    "print(\"Cosine similarity between 'woods' \" +\n",
    "          \"and 'sleep' - Skip Gram : \", \n",
    "    model2.similarity('woods', 'sleep')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'i' and 'he' - Skip Gram :  0.015586475\n"
     ]
    }
   ],
   "source": [
    "#Skip Gram: i, he\n",
    "print(\"Cosine similarity between 'i' \" +\n",
    "            \"and 'he' - Skip Gram : \", \n",
    "      model2.similarity('i', 'he')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What structure does Word2Vec convert text data into?\n",
    "    \n",
    "    It converts text data into vector space. Each unique word is assigned a corresponding vector in the space.\n",
    "    \n",
    "What is the Continuous Bag of Words Model?\n",
    "    \n",
    "    The Continous Bag of Words Model, or CBOW predicts the current words given context words within a specific window. \n",
    "    It represents words in an unordered fashion as vectors.\n",
    "What is cosine similarity?\n",
    "    \n",
    "    Cosine similarity measures the cosine angle between two vectors given by two similar words given in a text.\n",
    "    \n",
    "What is your interpretation of the cosine similarity results?\n",
    "    \n",
    "    Being that both similarity trials for (i,he) and (woods,sleep) are both near zero, they have little/no similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the Skip Gram Model?\n",
    "    \n",
    "    Skip Gram Model is the prediction of surrounding words in the text within a specific window given a current word.\n",
    "    \n",
    "What is your interpretation of the cosine similarity results?\n",
    "    \n",
    "    Given that both trials of Cosine Similarities for the Skip Gram Model are near 0, I can conclude that there is little \n",
    "    to no similarity within the specific window of surrounding words.\n",
    "    \n",
    "How do the Continuous Bag of Words results compare to the Skip Gram results?\n",
    "    \n",
    "    Both results are near similar. Although, the cosine similarity between (i,he) for the Skip Gram models has less similarity compared to the CBOW model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
